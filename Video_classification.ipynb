{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sMvbkPAS3MdF"
      },
      "outputs": [],
      "source": [
        "### Summary #####\n",
        "# 1. In this code , a video classification model has been created on UCF101 dataset. It has total 101 classes.\n",
        "# 2. Due to resource Constraint, For initial stage model has been trained on 10 classes using ResNet50 model.\n",
        "# 3. We get an \"pretrained_weights_c10.h5\" file in the first stage of training\n",
        "# 4. Now we want to add 5 more classes to these pretrained weights. A new dataset has been added named \"Custom_dataset_New_5_classes\"\n",
        "# 5. We load the pre-trained model trained on the existing 10 classes.\n",
        "# 6. We remove the original output layer of the pre-trained model since it only corresponds to the 10 classes.\n",
        "# 7. We freeze the layers of the pre-trained model to retain their weights.\n",
        "# 8. We add a new output layer with 15 units to accommodate the additional 5 classes.\n",
        "# 9. We train the modified model only on the new dataset containing the 5 new classes.\n",
        "# 10.Finally, we save the model, which now recognizes all classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "SHtAzIRAZbAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UkjZ9uY2-IjB"
      },
      "outputs": [],
      "source": [
        "os.chdir(r'/content/drive/MyDrive/Proglient_Assessment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "sPzkPs2M3Mfl"
      },
      "outputs": [],
      "source": [
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, num_frames=16, resize=(224, 224)):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=np.int16)\n",
        "\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if ret is False:\n",
        "            break\n",
        "        if i in frame_indices:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "hM1nGzDB3MiD"
      },
      "outputs": [],
      "source": [
        "# Path to the dataset directory\n",
        "# dataset_dir = '/content/drive/MyDrive/Proglient_Assessment/New_dataset_5_classes'\n",
        "\n",
        "dataset_dir='/content/drive/MyDrive/Proglient_Assessment/New_dataset_10_classes'\n",
        "\n",
        "# List to store frames and labels\n",
        "frames = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SYP-2AS_3MkW"
      },
      "outputs": [],
      "source": [
        "# Loop through each class directory\n",
        "for class_name in os.listdir(dataset_dir):\n",
        "    class_dir = os.path.join(dataset_dir, class_name)\n",
        "    # Loop through each video in the class directory\n",
        "    for video_name in os.listdir(class_dir):\n",
        "        video_path = os.path.join(class_dir, video_name)\n",
        "        extracted_frames = extract_frames(video_path)\n",
        "        frames.extend(extracted_frames)\n",
        "        labels.extend([class_name] * len(extracted_frames))\n",
        "\n",
        "# Convert frames and labels to numpy arrays\n",
        "frames = np.array(frames)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ufg4QdCS3Mmt"
      },
      "outputs": [],
      "source": [
        "# Perform one-hot encoding on the labels\n",
        "label_binarizer = LabelBinarizer()\n",
        "labels_encoded = label_binarizer.fit_transform(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DlOqLBIb3Mo8"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(frames, labels_encoded, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmP12Y9a3Mq-",
        "outputId": "b4f95a61-aa3a-4384-dd51-3eb2fc98ca5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5757, 224, 224, 3)\n",
            "(5757, 10)\n",
            "(1440, 224, 224, 3)\n",
            "(1440, 10)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YYSMbDHR3MtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904080c1-f66e-4aa8-ce14-43758a1ea79b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained ResNet50 model\n",
        "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax') ##no. of classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBEJ8bla7lca"
      },
      "outputs": [],
      "source": [
        "##### Training code###############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LBf-zXzw53St",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c79ff0-12a8-48b0-d693-abeaa215db68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "180/180 [==============================] - 58s 132ms/step - loss: 1.9911 - accuracy: 0.8755 - val_loss: 2.5285 - val_accuracy: 0.8139 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "180/180 [==============================] - 21s 114ms/step - loss: 0.3015 - accuracy: 0.9660 - val_loss: 73.3711 - val_accuracy: 0.5264 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.3437 - accuracy: 0.9653 - val_loss: 0.5500 - val_accuracy: 0.8951 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "180/180 [==============================] - 21s 114ms/step - loss: 0.5182 - accuracy: 0.9599 - val_loss: 97.6714 - val_accuracy: 0.6069 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.2971 - accuracy: 0.9732 - val_loss: 0.3490 - val_accuracy: 0.9340 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.4306 - accuracy: 0.9543 - val_loss: 0.2838 - val_accuracy: 0.9771 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "180/180 [==============================] - 21s 114ms/step - loss: 0.1099 - accuracy: 0.9868 - val_loss: 0.7195 - val_accuracy: 0.9507 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.1343 - accuracy: 0.9852 - val_loss: 0.1028 - val_accuracy: 0.9903 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.0898 - accuracy: 0.9889 - val_loss: 0.0465 - val_accuracy: 0.9937 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "180/180 [==============================] - 20s 114ms/step - loss: 0.0352 - accuracy: 0.9936 - val_loss: 0.1228 - val_accuracy: 0.9944 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "180/180 [==============================] - 21s 114ms/step - loss: 0.1152 - accuracy: 0.9870 - val_loss: 22.7798 - val_accuracy: 0.5056 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.0604 - accuracy: 0.9929 - val_loss: 0.0070 - val_accuracy: 0.9979 - lr: 2.0000e-04\n",
            "Epoch 13/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.0285 - accuracy: 0.9962 - val_loss: 0.0042 - val_accuracy: 0.9993 - lr: 2.0000e-04\n",
            "Epoch 14/50\n",
            "180/180 [==============================] - 21s 114ms/step - loss: 0.0154 - accuracy: 0.9977 - val_loss: 0.0050 - val_accuracy: 0.9993 - lr: 2.0000e-04\n",
            "Epoch 15/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.0035 - val_accuracy: 0.9993 - lr: 2.0000e-04\n",
            "Epoch 16/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.0030 - val_accuracy: 0.9993 - lr: 2.0000e-04\n",
            "Epoch 17/50\n",
            "180/180 [==============================] - 21s 114ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0036 - val_accuracy: 0.9972 - lr: 2.0000e-04\n",
            "Epoch 18/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 4.3682e-04 - accuracy: 0.9998 - val_loss: 0.0023 - val_accuracy: 0.9986 - lr: 2.0000e-04\n",
            "Epoch 19/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 7.0861e-04 - val_accuracy: 0.9993 - lr: 2.0000e-04\n",
            "Epoch 20/50\n",
            "180/180 [==============================] - 21s 114ms/step - loss: 0.0083 - accuracy: 0.9991 - val_loss: 0.0054 - val_accuracy: 0.9979 - lr: 2.0000e-04\n",
            "Epoch 21/50\n",
            "180/180 [==============================] - 21s 114ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0011 - val_accuracy: 0.9993 - lr: 2.0000e-04\n",
            "Epoch 22/50\n",
            "180/180 [==============================] - 21s 115ms/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 0.0042 - val_accuracy: 0.9986 - lr: 4.0000e-05\n",
            "45/45 [==============================] - 1s 31ms/step - loss: 7.0861e-04 - accuracy: 0.9993\n",
            "Test Loss: 0.0007086097612045705\n",
            "Test Accuracy: 0.9993055462837219\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=60, batch_size=32)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "# Save the weights\n",
        "model.save_weights('pretrained_weights_c10.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wndaYa_49-vL",
        "outputId": "15927232-35df-47e3-9a7f-bf446f5adc0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes in the dataset:\n",
            "0: ApplyEyeMakeup\n",
            "1: ApplyLipstick\n",
            "2: Archery\n",
            "3: BabyCrawling\n",
            "4: BalanceBeam\n",
            "5: BandMarching\n",
            "6: BaseballPitch\n",
            "7: Basketball\n",
            "8: BasketballDunk\n",
            "9: BenchPress\n"
          ]
        }
      ],
      "source": [
        "# Load the saved weights\n",
        "model.load_weights('pretrained_weights_c10.h5')\n",
        "\n",
        "# Get the classes from the label binarizer\n",
        "classes = label_binarizer.classes_\n",
        "\n",
        "# Print the classes\n",
        "print(\"Classes in the dataset:\")\n",
        "for i, class_name in enumerate(classes):\n",
        "    print(f\"{i}: {class_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predict classes for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "class_names = [\"class_0\", \"class_1\", \"class_2\", \"class_3\", \"class_4\", \"class_5\", \"class_6\", \"class_7\", \"class_8\", \"class_9\"]\n",
        "report = classification_report(y_true_classes, y_pred_classes, target_names=class_names)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88FHBvsY-Ib4",
        "outputId": "c5c571ea-d992-441c-ac80-34469d83bd73"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45/45 [==============================] - 2s 30ms/step\n",
            "Confusion Matrix:\n",
            "[[139   0   0   0   0   0   0   0   0   0]\n",
            " [  0 133   0   0   0   0   0   0   0   0]\n",
            " [  0   0 153   0   0   0   0   0   0   0]\n",
            " [  0   1   0 145   0   0   0   0   0   0]\n",
            " [  0   0   0   0 145   0   0   0   0   0]\n",
            " [  0   0   0   0   0 144   0   0   0   0]\n",
            " [  0   0   0   0   0   0 147   0   0   0]\n",
            " [  0   0   0   0   0   0   0 135   0   0]\n",
            " [  0   0   0   0   0   0   0   0 141   0]\n",
            " [  0   0   0   0   0   0   0   0   0 157]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       1.00      1.00      1.00       139\n",
            "     class_1       0.99      1.00      1.00       133\n",
            "     class_2       1.00      1.00      1.00       153\n",
            "     class_3       1.00      0.99      1.00       146\n",
            "     class_4       1.00      1.00      1.00       145\n",
            "     class_5       1.00      1.00      1.00       144\n",
            "     class_6       1.00      1.00      1.00       147\n",
            "     class_7       1.00      1.00      1.00       135\n",
            "     class_8       1.00      1.00      1.00       141\n",
            "     class_9       1.00      1.00      1.00       157\n",
            "\n",
            "    accuracy                           1.00      1440\n",
            "   macro avg       1.00      1.00      1.00      1440\n",
            "weighted avg       1.00      1.00      1.00      1440\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1io4LnV906Q"
      },
      "outputs": [],
      "source": [
        "# Inferencing to the unseen video#####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AaPB5SJK91BX"
      },
      "outputs": [],
      "source": [
        "unseen_video_path = '/content/drive/MyDrive/Proglient_Assessment/Testing_videos/v_ApplyEyeMakeup_g07_c06.avi'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgQpQ_Ge95iU",
        "outputId": "f3ce3314-ffde-4f50-8f64-e50ee582d535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "The predicted class for the unseen video is: class_0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBZjMeD-_HkH"
      },
      "outputs": [],
      "source": [
        "       ######## ADD THE CUSTOM MODEL WITH NEW CLASSES ############"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will now freeze the layers of the pre-trained model that were trained on the initial 10 classes,\n",
        "# add new layers to handle the new classes, and then train the entire model on the combined dataset (initial 10 classes + new 5 classes).\n",
        "# This way, the model retains the knowledge learned from the initial classes while also adapting to the new classes."
      ],
      "metadata": {
        "id": "eFYYZYpPCex-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CWasCnYc_Hrs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import glorot_uniform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved weights\n",
        "model.load_weights('pretrained_weights_c10.h5')"
      ],
      "metadata": {
        "id": "vXefWxEYHvqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "SONoifI7ItWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove the original output layer\n",
        "model.layers.pop()\n",
        "\n",
        "# Freeze the layers of the pretrained model\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "qoUiEASlIJeR"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, num_frames=16, resize=(224, 224)):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=np.int16)\n",
        "\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if ret is False:\n",
        "            break\n",
        "        if i in frame_indices:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ],
      "metadata": {
        "id": "eieH76fQI2If"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the New dataset directory\n",
        "\n",
        "new_dataset_dir='/content/drive/MyDrive/Proglient_Assessment/Custom_dataset_New_5_classes'\n",
        "\n",
        "# List to store frames and labels\n",
        "frames = []\n",
        "labels = []\n",
        "# Loop through each class directory\n",
        "for class_name in os.listdir(new_dataset_dir):\n",
        "    class_dir = os.path.join(new_dataset_dir, class_name)\n",
        "    # Loop through each video in the class directory\n",
        "    for video_name in os.listdir(class_dir):\n",
        "        video_path = os.path.join(class_dir, video_name)\n",
        "        extracted_frames = extract_frames(video_path)\n",
        "        frames.extend(extracted_frames)\n",
        "        labels.extend([class_name] * len(extracted_frames))\n",
        "\n",
        "# Convert frames and labels to numpy arrays\n",
        "frames = np.array(frames)\n",
        "labels = np.array(labels)\n",
        "# Perform one-hot encoding on the labels\n",
        "label_binarizer = LabelBinarizer()\n",
        "labels_encoded = label_binarizer.fit_transform(labels)\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(frames, labels_encoded, test_size=0.2, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85csnkTKJCfA",
        "outputId": "c4a2f5cc-74a9-4fd9-9a3a-f691e0f230f3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(958, 224, 224, 3)\n",
            "(958, 5)\n",
            "(240, 224, 224, 3)\n",
            "(240, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Concatenate"
      ],
      "metadata": {
        "id": "k0vUTfO_M88C"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new Dense layer for the new classes\n",
        "new_output = Dense(5, activation='softmax')(model.layers[-1].output)  # Assuming the last layer is Dense\n",
        "\n",
        "# Concatenate original output with new output\n",
        "new_output_concatenated = Dense(15, activation='softmax')(model.layers[-2].output)\n",
        "\n",
        "# Create the new model\n",
        "model = Model(inputs=model.input, outputs=new_output_concatenated)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "ytO2UGZrJfKi"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "# Save the weights\n",
        "model.save_weights('pretrained_weights_new15classes.h5')"
      ],
      "metadata": {
        "id": "8Pu4LHJ_K2_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbhSUweOQOOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAbo_7C5QOQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AEhIzVpUQOS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gHM5Y29_QOWh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}