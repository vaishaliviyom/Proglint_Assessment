{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Summary #####\n",
        " 1. In this code , a video classification model has been created on UCF101 dataset. It has total 101 classes.\n",
        " 2. Due to resource Constraint, For initial stage model has been trained on 10 classes using ResNet50 model.\n",
        " 3. We get an \"pretrained_weights_c10.h5\" file in the first stage of training\n",
        " 4. Now we want to add 5 more classes to these pretrained weights. A new dataset has been added named \"Custom_dataset_New_5_classes\"\n",
        " 5. We load the pre-trained model trained on the existing 10 classes.\n",
        " 6. We remove the original output layer of the pre-trained model since it only corresponds to the 10 classes.\n",
        " 7. We freeze the layers of the pre-trained model to retain their weights.\n",
        " 8. We add a new output layer with 15 units to accommodate the additional 5 classes.\n",
        " 9. We train the modified model only on the new dataset containing the 5 new classes.\n",
        " 10.Finally, we save the model, which now recognizes all classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "zDcaCjnP3JbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Important Libraries\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import glorot_uniform"
      ],
      "metadata": {
        "id": "SHtAzIRAZbAa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UkjZ9uY2-IjB"
      },
      "outputs": [],
      "source": [
        "os.chdir(r'/content/drive/MyDrive/Proglint_Assessment_2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvgFlbl8BvMR",
        "outputId": "9ada916a-61ba-4ba3-fb2e-b94a107c2a42"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sPzkPs2M3Mfl"
      },
      "outputs": [],
      "source": [
        "# Function to extract frames from videos\n",
        "\n",
        "def extract_frames(video_path, num_frames=16, resize=(224, 224)):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=np.int16)\n",
        "\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if ret is False:\n",
        "            break\n",
        "        if i in frame_indices:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the dataset ##\n",
        "Size of this dataset is 2 GB"
      ],
      "metadata": {
        "id": "MCuLlO8E4WOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hM1nGzDB3MiD"
      },
      "outputs": [],
      "source": [
        "# Path to the dataset directory\n",
        "\n",
        "dataset_dir='/content/drive/MyDrive/Proglint_Assessment/New_dataset_10_classes'\n",
        "\n",
        "# List to store frames and labels\n",
        "\n",
        "frames = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SYP-2AS_3MkW"
      },
      "outputs": [],
      "source": [
        "# Loop through each class directory\n",
        "\n",
        "for class_name in os.listdir(dataset_dir):\n",
        "    class_dir = os.path.join(dataset_dir, class_name)\n",
        "\n",
        "    # Loop through each video in the class directory\n",
        "\n",
        "    for video_name in os.listdir(class_dir):\n",
        "        video_path = os.path.join(class_dir, video_name)\n",
        "        extracted_frames = extract_frames(video_path)\n",
        "        frames.extend(extracted_frames)\n",
        "        labels.extend([class_name] * len(extracted_frames))\n",
        "\n",
        "# Convert frames and labels to numpy arrays\n",
        "frames = np.array(frames)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ufg4QdCS3Mmt"
      },
      "outputs": [],
      "source": [
        "# Perform one-hot encoding on the labels\n",
        "label_binarizer = LabelBinarizer()\n",
        "labels_encoded = label_binarizer.fit_transform(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DlOqLBIb3Mo8"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(frames, labels_encoded, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmP12Y9a3Mq-",
        "outputId": "99f597cc-819a-4a12-b7bb-5ff390a47089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5753, 224, 224, 3)\n",
            "(5753, 10)\n",
            "(1439, 224, 224, 3)\n",
            "(1439, 10)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YYSMbDHR3MtP"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained ResNet50 model\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# Build the model using regularization L2 Method\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # L2 regularization\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')  # Number of classes\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Training on 10 classes##\n",
        "\n",
        "> Got our first pretrained weights file from here\n",
        "\n"
      ],
      "metadata": {
        "id": "fotm6JnP3xcz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LBf-zXzw53St",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7751db69-b16f-4059-b9ab-a45c82596df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "203/203 [==============================] - 57s 122ms/step - loss: 7.3815 - accuracy: 0.8580 - val_loss: 173.9136 - val_accuracy: 0.6708 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 2.5382 - accuracy: 0.9611 - val_loss: 1.6233 - val_accuracy: 0.9958 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 2.2338 - accuracy: 0.9572 - val_loss: 2.3918 - val_accuracy: 0.8958 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "203/203 [==============================] - 22s 109ms/step - loss: 1.5901 - accuracy: 0.9737 - val_loss: 2.4674 - val_accuracy: 0.9056 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.9310 - accuracy: 0.9955 - val_loss: 0.6896 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 6/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.5604 - accuracy: 0.9986 - val_loss: 0.4371 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 7/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.3680 - accuracy: 0.9989 - val_loss: 0.2904 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 8/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.2375 - accuracy: 1.0000 - val_loss: 0.1912 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 9/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.1697 - accuracy: 0.9989 - val_loss: 0.1463 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 10/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.1205 - accuracy: 0.9998 - val_loss: 0.0956 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 11/50\n",
            "203/203 [==============================] - 22s 111ms/step - loss: 0.0906 - accuracy: 0.9994 - val_loss: 0.0760 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 12/50\n",
            "203/203 [==============================] - 22s 109ms/step - loss: 0.1421 - accuracy: 0.9975 - val_loss: 0.1072 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 13/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.0764 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 14/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.0495 - accuracy: 0.9997 - val_loss: 0.0569 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 15/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.0545 - accuracy: 0.9994 - val_loss: 0.0403 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 16/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.0415 - accuracy: 0.9995 - val_loss: 0.0333 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 17/50\n",
            "203/203 [==============================] - 22s 109ms/step - loss: 0.2257 - accuracy: 0.9946 - val_loss: 0.2211 - val_accuracy: 1.0000 - lr: 2.0000e-04\n",
            "Epoch 18/50\n",
            "203/203 [==============================] - 22s 110ms/step - loss: 0.1799 - accuracy: 0.9974 - val_loss: 0.1757 - val_accuracy: 0.9917 - lr: 2.0000e-04\n",
            "Epoch 19/50\n",
            "203/203 [==============================] - 22s 111ms/step - loss: 0.1374 - accuracy: 0.9994 - val_loss: 0.1164 - val_accuracy: 1.0000 - lr: 4.0000e-05\n",
            "23/23 [==============================] - 1s 31ms/step - loss: 0.0333 - accuracy: 1.0000\n",
            "Test Loss: 0.0333438441157341\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "# Save the weights\n",
        "model.save_weights('pretrained_weights_c10_new.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wndaYa_49-vL",
        "outputId": "53d7c6ed-ea68-41a3-8336-6b7314b6d37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes in the dataset:\n",
            "0: ApplyLipstick\n",
            "1: Archery\n",
            "2: BabyCrawling\n",
            "3: BalanceBeam\n",
            "4: BandMarching\n",
            "5: BaseballPitch\n",
            "6: Basketball\n",
            "7: BasketballDunk\n",
            "8: BenchPress\n",
            "9: Biking\n"
          ]
        }
      ],
      "source": [
        "# We will Load the saved weights now obtained after training\n",
        "\n",
        "model.load_weights('pretrained_weights_c10_new.h5')\n",
        "\n",
        "# Get the classes from the label binarizer\n",
        "classes = label_binarizer.classes_\n",
        "\n",
        "# # Print the classes\n",
        "print(\"Classes in the dataset:\")\n",
        "for i, class_name in enumerate(classes):     # There are intially 10 classes whose information is giving below\n",
        "    print(f\"{i}: {class_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predict classes for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "class_names = [\"class_0\", \"class_1\", \"class_2\", \"class_3\", \"class_4\", \"class_5\", \"class_6\", \"class_7\", \"class_8\", \"class_9\"]\n",
        "report = classification_report(y_true_classes, y_pred_classes, target_names=class_names)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88FHBvsY-Ib4",
        "outputId": "ee542368-4cdd-499e-bfce-887d1c38be76"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 1s 30ms/step\n",
            "Confusion Matrix:\n",
            "[[82  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 64  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 69  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 64  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 68  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 79  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 68  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 88  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 73  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 65]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       1.00      1.00      1.00        82\n",
            "     class_1       1.00      1.00      1.00        64\n",
            "     class_2       1.00      1.00      1.00        69\n",
            "     class_3       1.00      1.00      1.00        64\n",
            "     class_4       1.00      1.00      1.00        68\n",
            "     class_5       1.00      1.00      1.00        79\n",
            "     class_6       1.00      1.00      1.00        68\n",
            "     class_7       1.00      1.00      1.00        88\n",
            "     class_8       1.00      1.00      1.00        73\n",
            "     class_9       1.00      1.00      1.00        65\n",
            "\n",
            "    accuracy                           1.00       720\n",
            "   macro avg       1.00      1.00      1.00       720\n",
            "weighted avg       1.00      1.00      1.00       720\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Inferencing to the unseen video (Total 4 results presented)#"
      ],
      "metadata": {
        "id": "jy517sT8mO5z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgQpQ_Ge95iU",
        "outputId": "2b54969d-a815-45c6-bb8d-3136ae39e609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "The predicted class for the unseen video is: class_6\n"
          ]
        }
      ],
      "source": [
        "# First Inferencing\n",
        "\n",
        "unseen_video_path='/content/v_Basketball_g01_c01.avi'\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Second Inferencing\n",
        "\n",
        "unseen_video_path='/content/v_BabyCrawling_g01_c03.avi'\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hcfJdfTlkII",
        "outputId": "35292abe-6404-408b-ead6-bac591979125"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 915ms/step\n",
            "The predicted class for the unseen video is: class_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Third Inferencing\n",
        "\n",
        "unseen_video_path='/content/v_ApplyLipstick_g01_c01.avi'\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "25pTI6V9KBpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530396d8-5cd8-46f3-9169-d34c58e0e8ab"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "The predicted class for the unseen video is: class_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fourth Inferencing\n",
        "\n",
        "unseen_video_path='/content/v_Biking_g01_c01.avi'\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7rP8cHwno5L",
        "outputId": "cdf7f9d1-0b8a-42b3-ad68-5b6699d38a7c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "The predicted class for the unseen video is: class_9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train custom model with new  5 classes using pretrained weights of previous trained model\n",
        "#Dataset Size= 1Gb#\n"
      ],
      "metadata": {
        "id": "epYDhbxWoGoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will now freeze the layers of the pre-trained model that were trained on the initial 10 classes,\n",
        "# and then add new layers to handle the new classes.\n",
        "# add new layers to handle the new classes, and then train the entire model on the combined dataset (initial 10 classes + new 5 classes).\n",
        "# This way, the model retains the knowledge learned from the initial classes while also adapting to the new classes."
      ],
      "metadata": {
        "id": "eFYYZYpPCex-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I am Loading  the saved weights obtained after training the model on 10 classes previously\n",
        "\n",
        "model.load_weights('pretrained_weights_c10_new.h5')  # Load the weights of previously trained model on 10 classes"
      ],
      "metadata": {
        "id": "1aVOygQevYpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the original output layer to replace it with our new classes\n",
        "model.layers.pop()\n",
        "\n",
        "# Freezing the layers of the pretrained model. This will freeze the already trained model on 10 classes .\n",
        "\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "v8trP8nhvySA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, num_frames=16, resize=(224, 224)):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=np.int16)\n",
        "\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if ret is False:\n",
        "            break\n",
        "        if i in frame_indices:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ],
      "metadata": {
        "id": "a9rFB3xnwZPh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the dataset directory\n",
        "new_dataset_dir = '/content/drive/MyDrive/Proglint_Assessment/Custom_dataset_New_5_classes'\n",
        "\n",
        "# List to store frames and labels\n",
        "frames = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "DAbo_7C5QOQs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each class directory\n",
        "for class_name in os.listdir(new_dataset_dir):\n",
        "    class_dir = os.path.join(new_dataset_dir, class_name)\n",
        "    # Loop through each video in the class directory\n",
        "    for video_name in os.listdir(class_dir):\n",
        "        video_path = os.path.join(class_dir, video_name)\n",
        "        extracted_frames = extract_frames(video_path)\n",
        "        frames.extend(extracted_frames)\n",
        "        labels.extend([class_name] * len(extracted_frames))\n",
        "\n",
        "# Convert frames and labels to numpy arrays\n",
        "frames = np.array(frames)\n",
        "labels = np.array(labels)\n",
        "# Perform one-hot encoding on the labels\n",
        "label_binarizer = LabelBinarizer()\n",
        "labels_encoded = label_binarizer.fit_transform(labels)\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(frames, labels_encoded, test_size=0.2, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "AEhIzVpUQOS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d21a7d-f09e-4370-c3db-3fb887cc51e9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(958, 224, 224, 3)\n",
            "(958, 5)\n",
            "(240, 224, 224, 3)\n",
            "(240, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Again training of the 1GB dataset only by freezing the previous classes and including new 5 classes  ##\n",
        "\n",
        "Here we will add a new dense layer of 5 classes and concatenate it with previous layer to get all 15 classes"
      ],
      "metadata": {
        "id": "XY68CoZw5fKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new Dense layer for the new classes\n",
        "new_output = Dense(5, activation='softmax')(model.layers[-1].output)  # Assuming the last layer is Dense\n",
        "\n",
        "# Concatenate original output with new output\n",
        "new_output_concatenated = Dense(15, activation='softmax')(model.layers[-2].output)\n",
        "\n",
        "# Create the new model\n",
        "model = Model(inputs=model.input, outputs=new_output_concatenated)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jkgOXmZht3-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=7, batch_size=32)\n",
        "# #\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "# Save the weights\n",
        "model.save_weights('pretrained_weights_c15_new.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHt0fCL4w-QU",
        "outputId": "2d033703-0c9f-47e7-c337-b1ab95841769"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 45s 254ms/step - loss: 8.0672 - accuracy: 0.9134 - val_loss: 26224122.0000 - val_accuracy: 0.2250 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 6.6106 - accuracy: 0.9645 - val_loss: 63721762816.0000 - val_accuracy: 0.2250 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 4.7186 - accuracy: 0.9875 - val_loss: 9055174.0000 - val_accuracy: 0.2250 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 3.4276 - accuracy: 0.9916 - val_loss: 1938372.2500 - val_accuracy: 0.2292 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 2.7564 - accuracy: 0.9948 - val_loss: 43486.3477 - val_accuracy: 0.3250 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 2.4420 - accuracy: 0.9969 - val_loss: 10118.0801 - val_accuracy: 0.3458 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 1.9314 - accuracy: 0.9948 - val_loss: 691.4855 - val_accuracy: 0.7458 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 1.5336 - accuracy: 1.0000 - val_loss: 64.6842 - val_accuracy: 0.9625 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 1.2101 - accuracy: 1.0000 - val_loss: 12.3511 - val_accuracy: 0.9875 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 4s 127ms/step - loss: 0.9708 - accuracy: 1.0000 - val_loss: 2.9354 - val_accuracy: 0.9917 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.8547 - accuracy: 0.9990 - val_loss: 1.2283 - val_accuracy: 0.9958 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.9671 - accuracy: 1.0000 - val_loss: 0.9504 - val_accuracy: 0.9958 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.8083 - accuracy: 1.0000 - val_loss: 0.7222 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 4s 127ms/step - loss: 0.6564 - accuracy: 1.0000 - val_loss: 0.5917 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.5422 - accuracy: 1.0000 - val_loss: 0.4926 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.4536 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 4s 127ms/step - loss: 0.3824 - accuracy: 1.0000 - val_loss: 0.3503 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.3258 - accuracy: 1.0000 - val_loss: 0.2990 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.2795 - accuracy: 1.0000 - val_loss: 0.2579 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 4s 127ms/step - loss: 0.2399 - accuracy: 1.0000 - val_loss: 0.2215 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.2215 - accuracy: 1.0000\n",
            "Test Loss: 0.2214791178703308\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here Loading the final weights file\n",
        "\n",
        "The output layer of previously trained model will concatinated with the existing weights\n"
      ],
      "metadata": {
        "id": "EBZUF7156JbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the finally saved weights obtained after getting the new weight file\n",
        "\n",
        "model.load_weights('pretrained_weights_c15_new.h5')\n",
        "\n",
        "# # Get the classes from the label binarizer\n",
        "classes = label_binarizer.classes_\n",
        "#\n",
        "# # # Print the classes\n",
        "print(\"Classes in the dataset:\")\n",
        "for i, class_name in enumerate(classes):     # Now we will get all the 10 previous classes and 5 more new classes\n",
        "    print(f\"{i}: {class_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glruz8umyBGR",
        "outputId": "be882331-a913-450a-a25d-00a8e8bdb31e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes in the dataset:\n",
            "0: ApplyLipstick\n",
            "1: Archery\n",
            "2: BabyCrawling\n",
            "3: BalanceBeam\n",
            "4: BandMarching\n",
            "5: BaseballPitch\n",
            "6: Basketball\n",
            "7: BasketballDunk\n",
            "8: BenchPress\n",
            "9: Biking\n",
            "10: TennisSwing\n",
            "11: ThrowDiscus\n",
            "12: TrampolineJumping\n",
            "13: Typing\n",
            "14: UnevenBars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencing on all 15 classes using new weights ###\n",
        "# 5 example shown ##"
      ],
      "metadata": {
        "id": "1Zy7AF060iNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First Inferencing\n",
        "\n",
        "unseen_video_path='/content/v_Archery_g01_c02.avi'\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9', 10: 'class_10', 11: 'class_11',12: 'class_12',13: 'class_13',14: 'class_14',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2EfBSx-0hWL",
        "outputId": "65d9f5f4-0bce-4841-f82b-e2df9231c482"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 37ms/step\n",
            "The predicted class for the unseen video is: class_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second Inferencing\n",
        "\n",
        "unseen_video_path='/content/v_BalanceBeam_g04_c02.avi'\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9', 10: 'class_10', 11: 'class_11',12: 'class_12',13: 'class_13',14: 'class_14',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUqJQvBP0hYt",
        "outputId": "24951d71-97f7-4770-a52e-3952bf178b07"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n",
            "The predicted class for the unseen video is: class_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Third Inferencing\n",
        "\n",
        "unseen_video_path='/content/v_Typing_g01_c02.avi'\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9', 10: 'class_10', 11: 'class_11',12: 'class_12',13: 'class_13',14: 'class_14',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xXbIRbS0hcE",
        "outputId": "9b9552fa-a290-43f9-c374-3b7cf7726aef"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "The predicted class for the unseen video is: class_13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fourth Inferencing\n",
        "\n",
        "unseen_video_path='/content/v_UnevenBars_g04_c03.avi'\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9', 10: 'class_10', 11: 'class_11',12: 'class_12',13: 'class_13',14: 'class_14',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwU0Sgok2Z0x",
        "outputId": "9bee47fc-7d5f-4286-fa73-55fc13707018"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "The predicted class for the unseen video is: class_14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fifth Inferencing\n",
        "\n",
        "unseen_video_path='/content/v_TennisSwing_g01_c02.avi'\n",
        "# Extract frames from the unseen video\n",
        "unseen_frames = extract_frames(unseen_video_path)\n",
        "\n",
        "# Convert frames to numpy array and preprocess\n",
        "unseen_frames = np.array(unseen_frames) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(unseen_frames)\n",
        "\n",
        "# Aggregate predictions across frames\n",
        "final_prediction = np.argmax(np.sum(predictions, axis=0))\n",
        "\n",
        "# # Map prediction index to class label\n",
        "label_mapping = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3', 4: 'class_4', 5: 'class_5', 6: 'class_6',7: 'class_7', 8: 'class_8', 9: 'class_9', 10: 'class_10', 11: 'class_11',12: 'class_12',13: 'class_13',14: 'class_14',}\n",
        "predicted_class = label_mapping[final_prediction]\n",
        "\n",
        "print(f\"The predicted class for the unseen video is: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fobK1sYO2gzw",
        "outputId": "a3fe4633-1d22-4253-da7c-341215c36e8c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 37ms/step\n",
            "The predicted class for the unseen video is: class_10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KR7jNbyF2u_T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}